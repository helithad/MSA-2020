---
title: "MSA-2020"
author: "Helitha Dharmadasa"
date: "26/07/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(s20x)
library(mgcv)
library(MuMIn)
```

## Importing and Cleaning

```{r}
houses.df = read.table("Dataset_Final.csv", header=T, sep=",")
head(houses.df)
```


```{r}
summary(houses.df)

houses.df$Land.area = gsub("[^0-9]", "", houses.df$Land.area)
houses.df$Land.area = as.numeric(houses.df$Land.area)

for(i in 1:ncol(houses.df)){
  houses.df[is.na(houses.df[,i]), i] <- median(houses.df[,i], na.rm = TRUE)
}
```
We can see that the Land Area column is recorded as a string/char so we convert it to a numeric value by removing any non-numeric elements and converting with as.numeric(). We can see from the summary that there are 2 'NA' values in the bathrooms column, so we execute some code to replace any 'NA's found with the column median. As we run this after converting the Land Area column it will be checked as well. 


## Initial Analysis and Thoughts
```{r}
summary(houses.df)

```
All "NA's" are gone, and all columns that need to be numeric are now numeric. From a glance we can see that there are quite large max values and potential outliers in the bedrooms, year columns and in the Population column and especially the CV column. The rest of the data seems more naturally spread with maximums close to their 3rd quartiles and means and medians. We will have to keep an eye out on the variables with these potential outliers when looking at the cooks plot when creating our linear model.

```{r}
pairs20x(houses.df[c(5,1,2,4,6,7,8,9,10,11,12,13,14,16,17)])

```

We can try build a linear model that uses the other variables to estimate the capital value of the property. We can do some obvious transformations like logging the price as they are usually  Value doesn't seem to be too strongly correlated with anything this is supported by the distribution of CV above, population appears to have a strong correlation with quite a few other variables, Latitude and SA1 appear to have quite a strong correlation. CV doesn't appear to have a strong positive relationship with many other variables, at least at a glance. We can also include some guesses at potential interaction effects such as with bathrooms and bedrooms, and lat, long and land area. This is effectively going to be our worst case model to base the next section on.

## Fitting our Linear Model
```{r}
houses.fit <- lm(log(CV) ~ Bedrooms *  Bathrooms + Land.area * Latitude * Longitude + SA1 + X0.19.years + X20.29.years + X30.39.years + X40.49.years + X50.59.years + X60..years + Population + Deprivation.Index, data=houses.df)

#Methods and Assumptions checks
summary(houses.fit)
cooks20x(houses.fit)
normcheck(houses.fit)
eovcheck(houses.fit)

```

From this model we can see two things, one we have a lot of terms with poor significance and hence correlation in our model, and two there are some interaction effects present between some of or variables, specifically bathroom and bedrooms, and longitude and latitude and land area. We also have a few significant terms here we want to keep in the model. To simply the model down and hopefully improve its current poor R-squared value we can employ the MuMIn package. For the purposes of our methods and assumptions check we can see that normality looks good enough, EoV looks good and while there appears to be outliers at values 569, 732 and 567 in the cooks plot, they are within limits to leave in the model.

## Simplifying our Model

```{r}
options(na.action = "na.fail")

all.fits <- dredge(houses.fit)

head(all.fits)

```
The dredge function from MuMIn effectively tests every possible combination of variables present in our initial model to brute-force find the best simplification of our initial model based on AIcc values to compare models. All variables dropped are not significant went explaining CV.


```{r}
first.model <- get.models(all.fits,1)[[1]]
summary(first.model)

100*(exp(confint(first.model))-1)
```
Here we can see that in our best model produced by dredge all the terms included are now significant, and our R-squared improved marginally, though it will still be poor for prediction. This will be our final model.

##Methods and Assumptions Checks
From previous experience with price variables as well as analysis of the pairs plot it was reasoned that the CV variable should be log transformed. Following this we fit a 'worst case model' without overdoing it (i.e. fitting interactions on all terms) as a baseline for our dredge, which we used to automatically simplify the model. Normality can be seen to be satisfactory but not perfect, EoV lacks any obvious curvature or trend, and outliers in the cooks plot remain within limits to leave in the model.

##Executive Summary and Conclusions
My interest in this data was to see how the CV or capital value (i.e. price) of a property in NZ could be explained by the other variables in the data set.

Our model only 45% of the variance in the data and therefore is not suited for prediction.

The presence of interactions makes it difficult to calculate exact % increases, however generally the number of bathrooms and bedrooms increase value, strangely deprivation index being higher seems to decrease value, area seems to increase value, however the triple interaction with longitude and latitude here complicates matters, having people ages 0-19 decreases value by between 31.9 and 103%, ages 30-30 decreases value by 62.8 and 26.3% while ages 50-59 appear to increase value by 27 to 91%.













